{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t test\n",
    "def mean_confidence_interval(data, confidence = 0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * stats.t.ppf((1+confidence)/2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "#\n",
    "def mean_confidence_interval1(a, confidence = 0.95):\n",
    "    mean, sigma = np.mean(a), np.std(a)\n",
    "    #The 68% confidence interval for a single draw\n",
    "    stats.norm.interval(confidence, loc=mean, scale=sigma)\n",
    "    #The 68% confidence interval for the mean of N draws\n",
    "    return stats.norm.interval(confidence, loc=mean, scale=sigma/np.sqrt(len(a)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: choosing Invariant Metrics\n",
    "#### 1. Number of cookies: unique cookies to view the course overview page. this should be invariant metrics, because it is not affected by extra survey and should be the same among experiment and control group.\n",
    "\n",
    "#### 2. Number of user-ids: number of users who enroll in the free trial. this will decrease because some users may not enroll because of the commitment of 5hs.  but it along cannot be an evaluating metrics\n",
    "\n",
    "#### 3. Number of clicks: number of unique cookies to click the \"start free trial\" button (which happens before the free trial screener is trigger). this is not affected by the 5hs committment. should be invariant among experimental and control group\n",
    "\n",
    "#### 4.Click-through-probability: number of unique cookies to click the \"start free trial\" button divided by number of unique cookies to view the course overview. this is invariant, not affected by the 5hs committment. \n",
    "\n",
    "#### 5. Gross conversion: number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies clicked \"start free trials\". this number of users that enrolled will decrease if the user is not able to commit 5hs/per week, therefore this is an evaluation metric\n",
    "\n",
    "#### 6. retention: the number of user-ids remain enrolled past 14-day trial (at least made one payment) divide by the number of user-ids that complete checkout. also an evaluation metric\n",
    "\n",
    "#### 7. Net conversion: the number of user-ids remain enrolled past 14-day trial (at least made one payment) divide by the number of unique cookies clicked \"start free trials\". should decrease, an evaluation metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: choosing Evaluation Metrics\n",
    "\n",
    "#### 1. Number of cookies: unique cookies to view the course overview page. this should be invariant metrics, because it is not affected by extra survey and should be the same among experiment and control group.\n",
    "\n",
    "#### 2. Number of user-ids: number of users who enroll in the free trial. this will decrease because some users may not enroll because of the commitment of 5hs.  but it along cannot be an evaluating metrics\n",
    "\n",
    "#### 3. Number of clicks: number of unique cookies to click the \"start free trial\" button (which happens before the free trial screener is trigger). this is not affect by the \"start free trial\". should be invariant among experimental and control group\n",
    "\n",
    "#### 4.Click-through-probability: number of unique cookies to click the \"start free trial\" button divide by number of unique cookies to view the course overview. this is invariant, not affected by the button. \n",
    "\n",
    "#### 5. Gross conversion: number of user-ids to complete checkout and enroll in the free trial divided by the number of unique cookies clicked \"start free trials\". this number of users that enrolled will decrease if the user is not able to commit 5hs/per week, therefore this is an evaluation metric\n",
    "\n",
    "#### 6. retention: the number of user-ids remain enrolled past 14-day trial (at least made one payment) divide by the number of user-ids that complete checkout. also an evaluation metric\n",
    "\n",
    "#### 7. Net conversion: the number of user-ids remain enrolled past 14-day trial (at least made one payment) divide by the number of unique cookies clicked \"start free trials\". should decrease, an evaluation metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III. Measuring Variability\n",
    "#### For computing the variablility of the three metrics, e.g., gross conversion, retention, net conversion, all of which are probability type of metrics, we can assue the underlying distribution of the data to be binomial and use the estimated variance equation: (p_hat)(1-p_hat) / N. So the question changes to get N, the number of samples. We can follow the baseline values to estimate N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three standard errors : 0.020230604137049392, 0.05494901217850908, 0.01560154458248846\n"
     ]
    }
   ],
   "source": [
    "# Gross conversion: num user enrolled divide by num user click start free trials\n",
    "num_clicks = 5000\n",
    "p_click_through = 0.08 # baseline value\n",
    "num_start = num_clicks * p_click_through\n",
    "p_enroll_start = 0.20625 # baseline value : p enrolling, given click\n",
    "SE1 = np.sqrt(p_enroll_start * (1 - p_enroll_start)/num_start)\n",
    "\n",
    "# retention: num of user (one payment) paid divide by number user enrolled (some dropped after 14-day trial)\n",
    "num_enroll = num_start * p_enroll_start\n",
    "p_pay_enroll = 0.53 # baseline value: p of payment given enroll\n",
    "SE2 = np.sqrt(p_pay_enroll * (1- p_pay_enroll) / num_enroll)\n",
    "\n",
    "# Net conversion: num user paied divide by num user click start free trials\n",
    "\n",
    "num_paid1 = num_enroll * p_pay_enroll\n",
    "#or\n",
    "p_pay_start = 0.1093125\n",
    "num_paid2 = num_start * p_pay_start\n",
    "assert (num_paid1 == num_paid2)\n",
    "SE3 = np.sqrt(p_pay_start * (1- p_pay_start) / num_start)\n",
    "\n",
    "print(\"Three standard errors : {}, {}, {}\".format(SE1, SE2, SE3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Sizing\n",
    "I will not not use Bonferroni correction in the calculation\n",
    "The evaluation metrics that I select is gross conversion, retention, net conversion, and the pageviews needed is the largest among the three's. We can calculate from [blue_text](http://www.evanmiller.org/ab-testing/sample-size.htm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12850\n",
      "19552\n",
      "7642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2369939.393939394"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def z_score(alpha):\n",
    "    return -stats.norm.ppf(alpha/2.0) #qnorm\n",
    "\n",
    "def get_size(baseline_conversion, N, d_min, alpha, beta):\n",
    "    SE1 = np.sqrt(baseline_conversion * (1- baseline_conversion))\n",
    "    zscore = z_score(alpha)\n",
    "    for n in range(N):\n",
    "        SE = SE1/np.sqrt(n+1)\n",
    "        if stats.norm.cdf(SE * zscore, d_min, SE) <= beta:\n",
    "            return n+1\n",
    "    return -1\n",
    "\n",
    "\n",
    "#gross conversion\n",
    "baseline_conversion = 0.20625 # from doc description https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True\n",
    "d_min = 0.01 #practical significance level or Minimum Detectable Effect\n",
    "alpha = 0.05 # significance level\n",
    "beta = 0.2 # statistical power = 1 - beta = 0.8\n",
    "\n",
    "n1 = get_size(baseline_conversion, 500000, 0.01, 0.05, 0.2)\n",
    "\n",
    "print(n1)\n",
    "#retention\n",
    "baseline_conversion = 0.53 # from doc description https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True\n",
    "d_min = 0.01 #practical significance level or Minimum Detectable Effect\n",
    "alpha = 0.05 # significance level\n",
    "beta = 0.2 # statistical power = 1 - beta = 0.8\n",
    "\n",
    "n2 = get_size(baseline_conversion, 500000, 0.01, 0.05, 0.2)\n",
    "\n",
    "print(n2)\n",
    "\n",
    "#net conversion\n",
    "baseline_conversion = 0.1093125 # from doc description https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True\n",
    "d_min = 0.0075 #practical significance level or Minimum Detectable Effect\n",
    "alpha = 0.05 # significance level\n",
    "beta = 0.2 # statistical power = 1 - beta = 0.8\n",
    "\n",
    "n3 = get_size(baseline_conversion, 500000, 0.01, 0.05, 0.2)\n",
    "\n",
    "print(n3)\n",
    "\n",
    "#\n",
    "cookies = 40000\n",
    "num_start = 3200\n",
    "num_enrolled  = 660\n",
    "sample_size_two_groups1 = n1 * 2\n",
    "sample_size_two_groups2 = n2 * 2\n",
    "sample_size_two_groups3 = n3 * 2\n",
    "\n",
    "pageview1 = sample_size_two_groups1 / (num_start/cookies)\n",
    "pageview2 = sample_size_two_groups2 / (num_enrolled/cookies)\n",
    "pageview3 = sample_size_two_groups3 / (num_start/cookies)\n",
    "\n",
    "max(max(pageview1,pageview3), pageview2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing Duration vs. Exposure\n",
    "#### The duration of time can be calucalted by dividing the largest pageview by the cookies per day time the fraction of traffic the if we use 100% traffic, the duration last 119 days, which is quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Control  Experiment     Total  Prob    StdErr    margin  \\\n",
      "cookies      345543.0    344660.0  690203.0   0.5  0.000602  0.001180   \n",
      "clicks        28378.0     28325.0   56703.0   0.5  0.002100  0.004115   \n",
      "enrollments    3785.0      3423.0    7208.0   0.5  0.005889  0.011543   \n",
      "payments       2033.0      1945.0    3978.0   0.5  0.007928  0.015538   \n",
      "\n",
      "                low_b    high_b   obs_res   pass  \n",
      "cookies      0.498820  0.501180  0.499360   True  \n",
      "clicks       0.495885  0.504115  0.499533   True  \n",
      "enrollments  0.488457  0.511543  0.474889  False  \n",
      "payments     0.484462  0.515538  0.488939   True  \n",
      "0.08126698684411665 0.08309789448821087 0.08218244066616376\n"
     ]
    }
   ],
   "source": [
    "### Sanity check\n",
    "cont = pd.read_csv(\"Final Project Results - Control.csv\", sep = \",\")\n",
    "exp = pd.read_csv(\"Final Project Results - Experiment.csv\", sep = \",\")\n",
    "result = {\"Control\":[cont.Pageviews.sum(), cont.Clicks.sum(), cont.Enrollments.sum(), cont.Payments.sum()], \"Experiment\":[exp.Pageviews.sum(), exp.Clicks.sum(), exp.Enrollments.sum(), exp.Payments.sum()]}\n",
    "result = pd.DataFrame(result, index = [\"cookies\",\"clicks\",\"enrollments\",\"payments\"])\n",
    "result[\"Total\"] = result.Control + result.Experiment\n",
    "z = z_score(0.05)\n",
    "result['Prob'] = 0.5\n",
    "result[\"StdErr\"] = np.sqrt((result.Prob * (1-result.Prob)/result.Total))\n",
    "result['margin'] = z * result.StdErr\n",
    "result['low_b'] = result.Prob - result.margin\n",
    "result['high_b'] = result.Prob + result.margin\n",
    "result['obs_res'] = result.Experiment / result.Total\n",
    "result['pass'] = result.apply(lambda x: True if x.obs_res < x.high_b and x.obs_res > x.low_b else False, axis = 1)\n",
    "print(result)\n",
    "num_click_cont = result.loc[\"clicks\", \"Control\"]\n",
    "num_cookies_cont = result.loc[\"cookies\", \"Control\"]\n",
    "num_click_exp = result.loc[\"clicks\", \"Experiment\"]\n",
    "num_cookies_exp = result.loc[\"cookies\", \"Experiment\"]\n",
    "#control\n",
    "p_hat_click_through_cont = num_click_cont / num_cookies_cont\n",
    "#observed value\n",
    "p_hat_click_through_exp = num_click_exp / num_cookies_exp\n",
    "#standard error\n",
    "SE_click_through = np.sqrt((1-p_hat_click_through_cont) * p_hat_click_through_cont / num_cookies_cont)\n",
    "#margin for 95% confidence interval (z = 1.96)\n",
    "m_click_through = 1.96 * SE_click_through\n",
    "#lower-bound\n",
    "low_b = p_hat_click_through_exp - m_click_through\n",
    "upper_b = p_hat_click_through_exp + m_click_through\n",
    "print(low_b, upper_b, p_hat_click_through_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pooled stats\n",
    "def cal_stats(p_hat_total, z, N_cont, N_exp, p_hat_exp, p_hat_cont):\n",
    "    SE = np.sqrt(p_hat_total * (1 - p_hat_total) * (1 / N_cont + 1 / N_exp))\n",
    "    margin = z * SE\n",
    "    diff = p_hat_exp - p_hat_cont\n",
    "    low_b = diff - margin\n",
    "    upper_b = diff + margin\n",
    "    return SE, margin, low_b, upper_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Experiment   Control     Total   Diff\n",
      "cookies        211362.0  212163.0  423525.0 -801.0\n",
      "clicks          17260.0   17293.0   34553.0  -33.0\n",
      "enrollments      3423.0    3785.0    7208.0 -362.0\n",
      "payments         1945.0    2033.0    3978.0  -88.0\n",
      "0.004371675385225936 0.011729780091389183 0.0034341335129324238\n",
      "-0.01198639082531873 0.05408517368626556 0.001857179010803383\n",
      "-0.0291233583354044 0.008104435728019967 -0.011604624359891718\n"
     ]
    }
   ],
   "source": [
    "#### check for practical and statistical significance\n",
    "cont1 = cont.dropna(axis = 0).drop(['Date'], axis = 1)\n",
    "exp1 = exp.dropna(axis = 0).drop(['Date'], axis = 1)\n",
    "cont1 = np.sum(cont1, axis = 0)\n",
    "exp1 = np.sum(exp1, axis = 0)\n",
    "result1 = pd.DataFrame((exp1, cont1), index = ['Experiment', \"Control\"]).T\n",
    "result1.index = [\"cookies\",\"clicks\",\"enrollments\",\"payments\"]\n",
    "result1['Total'] = result1.Experiment + result1.Control\n",
    "result1['Diff'] = result1.Experiment - result1.Control\n",
    "print(result1)\n",
    "\n",
    "# Gross conversion : number of enrolled / number of clicks\n",
    "gross_conversion_exp = result1.loc['enrollments', 'Experiment'] / result1.loc['clicks', 'Experiment'] \n",
    "gross_conversion_cont = result1.loc['enrollments', 'Control'] / result1.loc['clicks', 'Control']\n",
    "gross_conversion_total = (result1.loc['enrollments', 'Experiment'] + result1.loc['enrollments', 'Control']) / (result1.loc['clicks', 'Experiment'] + result1.loc['clicks', 'Control'])\n",
    "se1, m1, lb1, ub1 = cal_stats(gross_conversion_total, 1.96, result1.loc['clicks', 'Control'], result1.loc['clicks', 'Experiment'], gross_conversion_exp, gross_conversion_cont)\n",
    "#print(gross_conversion_exp, gross_conversion_cont, gross_conversion_total )\n",
    "# Retention : number of paid / number of enrolled\n",
    "retention_exp = result1.loc['payments', 'Experiment'] / result1.loc['enrollments', 'Experiment'] \n",
    "retention_cont = result1.loc['payments', 'Control'] / result1.loc['enrollments', 'Control']\n",
    "retention_total = (result1.loc['payments', 'Experiment'] + result1.loc['payments', 'Control']) / (result1.loc['enrollments', 'Experiment'] + result1.loc['enrollments', 'Control'])\n",
    "se2, m2, lb2, ub2 = cal_stats(retention_total, 1.96, result1.loc['enrollments', 'Control'], result1.loc['enrollments', 'Experiment'], retention_exp, retention_cont)\n",
    "\n",
    "# Net conversion: number of paid / number of clicks\n",
    "net_conversion_exp = result1.loc['payments', 'Experiment'] / result1.loc['clicks', 'Experiment'] \n",
    "net_conversion_cont = result1.loc['payments', 'Control'] / result1.loc['clicks', 'Control']\n",
    "gross_conversion_total = (result1.loc['payments', 'Experiment'] + result1.loc['payments', 'Control']) / (result1.loc['clicks', 'Experiment'] + result1.loc['clicks', 'Control'])\n",
    "se3, m3, lb3, ub3 = cal_stats(gross_conversion_total, 1.96, result1.loc['clicks', 'Control'], result1.loc['clicks', 'Experiment'], net_conversion_exp, net_conversion_cont)\n",
    "\n",
    "\n",
    "print(se1, se2, se3)\n",
    "print(ub1, ub2, ub3)\n",
    "print(lb1, lb2, lb3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Sign tests\n",
    "#### here we need to analyze the gross/net conversion comparison on a daily basis and check how many days that gives a positive difference between the experiment group and control group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Pageviews_x  Clicks_x  Enrollments_x  Payments_x  Pageviews_y  \\\n",
      "0  Sat, Oct 11         7723       687          134.0        70.0         7716   \n",
      "1  Sun, Oct 12         9102       779          147.0        70.0         9288   \n",
      "2  Mon, Oct 13        10511       909          167.0        95.0        10480   \n",
      "3  Tue, Oct 14         9871       836          156.0       105.0         9867   \n",
      "4  Wed, Oct 15        10014       837          163.0        64.0         9793   \n",
      "\n",
      "   Clicks_y  Enrollments_y  Payments_y  gross_conversion_exp  \\\n",
      "0       686          105.0        34.0              0.153061   \n",
      "1       785          116.0        91.0              0.147771   \n",
      "2       884          145.0        79.0              0.164027   \n",
      "3       827          138.0        92.0              0.166868   \n",
      "4       832          140.0        94.0              0.168269   \n",
      "\n",
      "   gross_conversion_cont  net_conversion_exp  net_conversion_cont  \n",
      "0               0.195051            0.323810             0.522388  \n",
      "1               0.188703            0.784483             0.476190  \n",
      "2               0.183718            0.544828             0.568862  \n",
      "3               0.186603            0.666667             0.673077  \n",
      "4               0.194743            0.671429             0.392638  \n",
      "23 4 13\n"
     ]
    }
   ],
   "source": [
    "# print(cont)\n",
    "# print(exp)\n",
    "result2 = pd.merge(cont, exp, on = \"Date\")\n",
    "result2 = result2.dropna(axis = 0)\n",
    "\n",
    "result2[\"gross_conversion_exp\"] = result2.Enrollments_y / result2.Clicks_y\n",
    "result2[\"gross_conversion_cont\"] = result2.Enrollments_x / result2.Clicks_x\n",
    "result2[\"net_conversion_exp\"] = result2.Payments_y / result2.Enrollments_y\n",
    "result2[\"net_conversion_cont\"] = result2.Payments_x / result2.Enrollments_x\n",
    "\n",
    "# check how many days that there is an increase in experiment group in gross conversion, same with net conversion\n",
    "len_total = result2.shape[0]\n",
    "len_pos_gross = len(result2[result2.gross_conversion_exp > result2.gross_conversion_cont])\n",
    "len_pos_net = len(result2[result2.net_conversion_exp > result2.net_conversion_cont])\n",
    "print(result2.head())\n",
    "print(len_total, len_pos_gross, len_pos_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"\"\"-0.05\n",
    "0\n",
    "0.09\n",
    "-0.02\n",
    "-0.01\n",
    "0.04\n",
    "-0.01\n",
    "0\n",
    "0.02\n",
    "-0.06\n",
    "0.05\n",
    "0.01\n",
    "0.02\n",
    "0.02\n",
    "-0.03\n",
    "0\n",
    "0.04\n",
    "0.01\n",
    "0.04\n",
    "0.02\n",
    "-0.02\n",
    "0.03\n",
    "-0.01\n",
    "0.01\n",
    "-0.02\n",
    "0.06\n",
    "0.02\n",
    "-0.01\n",
    "0.04\n",
    "-0.02\n",
    "0.03\n",
    "0\n",
    "-0.06\n",
    "-0.02\n",
    "-0.01\n",
    "0.08\n",
    "-0.08\n",
    "-0.03\n",
    "0.02\n",
    "-0.02\"\"\"\n",
    "input_string = input_string.splitlines()\n",
    "input_string = [float(i) for i in input_string]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref:\n",
    "1. https://towardsdatascience.com/a-summary-of-udacity-a-b-testing-course-9ecc32dedbb1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
